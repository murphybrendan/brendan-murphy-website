---
title: "TWIL: Linear Algebra (for ML)"
categories:
  - blog
tags:
  - TWIL
  - linear algebra
---

I'm starting this series in order to consolidate everything I've learned during the week with regards to AI Safety. I think it could be useful for me, as there's a lot of support out there for the idea that you truly understand someting once you try to explain it to someone else. So, let's go.

I wanted to start with an area that I feel particularly weak in: linear algebra. I had a really unfortunate experience in college with it. It was so bad that, one week when we had a tutor for a substitute, she told the class that she had gotten a lot of requests to just continue teaching the course over him. I eventually stopped going to class and binged Khan Academy in order to do the homework and tests, but I retained absolutely nothing from it. And now that's biting me in the ass since so much of machine learning is built on linear algebra.

Thankfully, there's one resource out there that I found made everything click: 3blue1brown's Essence of Linear Algebra series. https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab. I cross-referenced ARENA 3.0 prereqs https://arena3-chapter0-fundamentals.streamlit.app/[0.0]_Prerequisites#linear-algebra in order to focus on only the topics that are directly relevant to machine learning.

Linear Transformations

What are linear transformations?

For the sake of simplicity, let's stick to 2D space. There are two unit vectors: $\hat{i} = \[1 \\ 0\]$ which is the vector that represents moving by 1 on the x axis, and $\hat{j} = \[0 \\ 1\]$ which is the vector that represents moving by 1 on the y axis. $\hat{i}$ and $\hat{j}$ are said to _span_ 2D space, which means that you can get to any point by simply taking some combination of the two. For example, if you want to get to the point $\(2, 3)\)$, you can think of it as combination of some number of $\hat{i}$s and $\hat{j}$s, in this case $\[2 \\ 3\] = 2*\hat{i} + 3*\hat{j} = 2*\[1 \\ 0\] + 3*\[0 \\ 1\]$.

Okay so we have two vectors $\hat{i}$ and $\hat{j}$, now what are linear transformations and how are these two unit vectors important?
A _linear transformation_ is just a function that maps vectors (or matrices) to vectors (or matrices) (I'm just going to stick to vectors for the sake of simplicity). The word _transformation_ suggests that that the vector _moves_ through space from the input to the output. Linear transformations have two important properties:
1. Grid lines remain straight and evenly-spaced. One way to visualize something that _isn't_ a linear transformation would be to turn the 2D grid into a bullseye.
2. The origin $\[0 \\ 0\]$ remains fixed in place. So $f(\vec{0}) = \vec{0}$. This means that the 2D grid isn't getting shifted in any direction.


Here's an example of a linear transformation in 2D:

$$A = \[1 & 1 \\ -1 & 2\]$$

If we want to 


I think a cool  way to think about linear transformations is that they express how to _warp_ space. In a 2D setting, you might have a linear transformation $\[1 & 1 \\ -1 & 2\]$ 

